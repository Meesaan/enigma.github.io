---
title: "Classification in View: Iris Dataset"
category: ML
date: 2015-02-18
header:
  image: "/images/int1.jpg"
excerpt: "Data Science, Supervised Learning, Unsupervised Learning"
---

## Classification in View: Iris Dataset

### 1. Intouction
This study describes the intuitions behind some of the the popular classification models. They include; Logistic regression, support vector machine, naive-bayes, Decision trees and Ensemble learning. Without further ado, lets dive in.  
Of course we have to import.


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
```


```python
from sklearn.datasets import load_iris
iris = load_iris()
iris.keys()
```




    dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])




```python
X = iris.data
y = iris.target
X.shape, y.shape
```




    ((150, 4), (150,))




```python
sns.FacetGrid(iris, hue="Species", palette="husl", size=5) \
   .map(plt.scatter, "SepalLengthCm", "SepalWidthCm") \
   .add_legend()
```


```python
def sklearn_to_df(sklearn_dataset):
    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)
    df['target'] = pd.Series(sklearn_dataset.target)
    return df

df_iris = sklearn_to_df(iris)
```


```python
df_iris.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_iris.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
    </tr>
    <tr>
      <td>mean</td>
      <td>5.843333</td>
      <td>3.057333</td>
      <td>3.758000</td>
      <td>1.199333</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>std</td>
      <td>0.828066</td>
      <td>0.435866</td>
      <td>1.765298</td>
      <td>0.762238</td>
      <td>0.819232</td>
    </tr>
    <tr>
      <td>min</td>
      <td>4.300000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>0.100000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>5.100000</td>
      <td>2.800000</td>
      <td>1.600000</td>
      <td>0.300000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>5.800000</td>
      <td>3.000000</td>
      <td>4.350000</td>
      <td>1.300000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>6.400000</td>
      <td>3.300000</td>
      <td>5.100000</td>
      <td>1.800000</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <td>max</td>
      <td>7.900000</td>
      <td>4.400000</td>
      <td>6.900000</td>
      <td>2.500000</td>
      <td>2.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```


```python
X_train.shape
```




    (120, 4)



### 3. Model Training

### 3.1. Logistic Regression
Logistic regression predicts a target variable in a boolean order. That is, it tells us if our prediction is true or false. Unlike linear regression that fits a line to the data, logistic regression fits an 'S' shaped logistic function. The sigmoid function, also called logistic function gives an ‘S’ shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO.  
Linear regression is estimated using Ordinary Least Squares (OLS) while logistic regression is estimated using Maximum Likelihood Estimation (MLE) approach.
<img src='logistic.png'>
The MLE is a "likelihood" maximization method, while OLS is a distance-minimizing approximation method. Maximizing the likelihood function determines the parameters that are most likely to produce the observed data. From a statistical point of view, MLE sets the mean and variance as parameters in determining the specific parametric values for a given model. This set of parameters can be used for predicting the data needed in a normal distribution.  
In a nutshell, MLE works by picking a probability scaled by X after observing y just like the 'S' curve above. Next that probability is used to estimate the likelihood of observing all of X. Finally, all these likelihoods are then multiplied together. The final estimate is the likelihood of the data given the initial 'S' line. Next, the line is shifted over and over again and the new likelihoods are calculated. Finally, the curve wit the maximum likelihood is selected.  
Logistic Regression (also called Logit
Regression) is commonly used to estimate the probability that an instance
belongs to a particular class (e.g., what is the probability that this email is
spam?). If the estimated probability is greater than 50%, then the model
predicts that the instance belongs to that class (called the positive class,
labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to
the negative class, labeled “0”). This makes it a binary classifier.  
So how does Logistic Regression work? Just like a Linear Regression model,
a Logistic Regression model computes a weighted sum of the input features
(plus a bias term), but instead of outputting the result directly like the
Linear Regression model does, it outputs the logistic of this result
(see Equation 4-13).
Equation 4-13. Logistic Regression model estimated probability (vectorized form)
pˆ=hθ(x)=σ(x⊺θ)p^=hθ(x)=σ(x⊺θ)  
The logistic—noted σ(·)—is a sigmoid function (i.e., S-shaped) that outputs
a number between 0 and 1.  
Once the Logistic Regression model has estimated the
probability pˆp^ = hθ(x) that an instance x belongs to the positive class, it
can make its prediction ŷ easily (see Equation 4-15).
Equation 4-15. Logistic Regression model prediction
yˆ={01ifpˆ<0.5ifpˆ≥0.5y^=0ifp^<0.51ifp^≥0.5
Notice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic
Regression model predicts 1 if x⊺ θ is positive and 0 if it is negative.  
The score t is often called the logit. The name comes from the fact that the
logit function, defined as logit(p) = log(p / (1 – p)), is the inverse of the
logistic function. Indeed, if you compute the logit of the estimated
probability p, you will find that the result is t. The logit is also called the logodds, since it is the log of the ratio between the estimated probability for
the positive class and the estimated probability for the negative class.  
Now you know how a Logistic Regression model estimates probabilities
and makes predictions. But how is it trained? The objective of training is to
set the parameter vector θ so that the model estimates high probabilities
for positive instances (y = 1) and low probabilities for negative instances
(y = 0). This idea is captured by the cost function shown in Equation 4-
16 for a single training instance x.
Equation 4-16. Cost function of a single training instance
c(θ)={−log(pˆ)−log(1−pˆ)if y=1if y=0c(θ)=-log(p^)if y=1-log(1-p^)if y=0
This cost function makes sense because –log(t) grows very large
when t approaches 0, so the cost will be large if the model estimates a
probability close to 0 for a positive instance, and it will also be very large if
the model estimates a probability close to 1 for a negative instance. On the
other hand, –log(t) is close to 0 when t is close to 1, so the cost will be close
to 0 if the estimated probability is close to 0 for a negative instance or close
to 1 for a positive instance, which is precisely what we want.
The cost function over the whole training set is the average cost over all
training instances. It can be written in a single expression called the log
loss, shown in Equation 4-17.
Equation 4-17. Logistic Regression cost function (log loss)
J(θ)=−1m∑mi=1[y(i)log(pˆ(i))+(1−y(i))log(1−pˆ(i))]J(θ)=-
1m∑i=1my(i)logp^(i)+(1-y(i))log1-p^(i)
The bad news is that there is no known closed-form equation to compute
the value of θ that minimizes this cost function (there is no equivalent of
the Normal Equation). The good news is that this cost function is convex, so
Gradient Descent (or any other optimization algorithm) is guaranteed to
find the global minimum (if the learning rate is not too large and you wait
long enough). The partial derivatives of the cost function with regard to
the j
th model parameter θj are given by Equation 4-18.
Equation 4-18. Logistic cost function partial derivatives
∂∂θjJ(θ)=1m∑i=1m(σ(θ⊺x(i))−y(i))x(i)j∂∂θjJ(θ)=1m∑i=1mσ(θ⊺x(i))-y(i)xj(i)
This equation looks very much like Equation 4-5: for each instance it
computes the prediction error and multiplies it by the j
th feature value, and
then it computes the average over all training instances. Once you have the
gradient vector containing all the partial derivatives, you can use it in the
Batch Gradient Descent algorithm. That’s it: you now know how to train a
Logistic Regression model. For Stochastic GD you would take one instance
at a time, and for Mini-batch GD you would use a mini-batch at a time.



```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

model_log = LogisticRegression(solver='lbfgs', max_iter=500,  multi_class='multinomial')
model_log.fit(X_train, y_train)
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                       intercept_scaling=1, l1_ratio=None, max_iter=500,
                       multi_class='multinomial', n_jobs=None, penalty='l2',
                       random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                       warm_start=False)




```python
predictions = model_log.predict(X_test)
print(predictions)# printing predictions

print()# Printing new line

#Check precision, recall, f1-score
print( classification_report(y_test, predictions) )

print( accuracy_score(y_test, predictions))
```

    [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0]

                  precision    recall  f1-score   support

               0       1.00      1.00      1.00        11
               1       1.00      1.00      1.00        13
               2       1.00      1.00      1.00         6

        accuracy                           1.00        30
       macro avg       1.00      1.00      1.00        30
    weighted avg       1.00      1.00      1.00        30

    1.0



```python
from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(y_test, predictions)
cnf_matrix
```




    array([[11,  0,  0],
           [ 0, 13,  0],
           [ 0,  0,  6]], dtype=int64)



### 3.2. Support Vector Machhines
A Support Vector Machine (SVM) is a powerful and versatile Machine
Learning model, capable of performing linear or nonlinear classification,
regression, and even outlier detection. It is one of the most popular models in Machine Learning, and anyone interested in Machine Learning should have it in their toolbox.  
The fundamental idea behind SVMs is best explained with some
pictures. Figure 5-1 shows part of the iris dataset that was introduced at
the end of Chapter 4. The two classes can clearly be separated easily with a
straight line (they are linearly separable). The left plot shows the decision
boundaries of three possible linear classifiers. The model whose decision
boundary is represented by the dashed line is so bad that it does not even
separate the classes properly. The other two models work perfectly on this
training set, but their decision boundaries come so close to the instances
that these models will probably not perform as well on new instances. In
contrast, the solid line in the plot on the right represents the decision
boundary of an SVM classifier; this line not only separates the two classes
but also stays as far away from the closest training instances as possible.
You can think of an SVM classifier as fitting the widest possible street
(represented by the parallel dashed lines) between the classes. This is
called large margin classification.  
<img src = 'svm.png'>
Notice that adding more training instances “off the street” will not affect
the decision boundary at all: it is fully determined (or “supported”) by the
instances located on the edge of the street. These instances are called
the support vectors (they are circled in Figure 5-1).  
<img src = 'svm2.png'>
SVMs are sensitive to the feature scales, as you can see in Figure 5-2: in the
left plot, the vertical scale is much larger than the horizontal scale, so the
widest possible street is close to horizontal. After feature scaling (e.g., using
Scikit-Learn’s StandardScaler), the decision boundary in the right plot looks
much better.  
If we strictly impose that all instances must be off the street and on the
right side, this is called hard margin classification. There are two main
issues with hard margin classification. First, it only works if the data is
linearly separable. Second, it is sensitive to outliers. Figure 5-3 shows the
iris dataset with just one additional outlier: on the left, it is impossible to
find a hard margin; on the right, the decision boundary ends up very
different from the one we saw in Figure 5-1 without the outlier, and it will
probably not generalize as well.  
To avoid these issues, use a more flexible model. The objective is to find a
good balance between keeping the street as large as possible and limiting
the margin violations (i.e., instances that end up in the middle of the street
or even on the wrong side). This is called soft margin classification.
When creating an SVM model using Scikit-Learn, we can specify a number
of hyperparameters. C is one of those hyperparameters. If we set it to a low
value, then we end up with the model on the left of Figure 5-4. With a high
value, we get the model on the right. Margin violations are bad. It’s usually
better to have few of them. However, in this case the model on the left has a
lot of margin violations but will probably generalize better.  
If your SVM model is overfitting, you can try regularizing it by reducing C.
The following Scikit-Learn code loads the iris dataset, scales the features,
and then trains a linear SVM model (using the LinearSVC class with C=1 and
the hinge loss function, described shortly) to detect Iris virginica flowers:
import numpy as np
from sklearn import datasets

with SGDClassifier(loss="hinge", alpha=1/(m*C)). This applies regular
Stochastic Gradient Descent (see Chapter 4) to train a linear SVM classifier.
It does not converge as fast as the LinearSVC class, but it can be useful to
handle online classification tasks or huge datasets that do not fit in memory
(out-of-core training).
TIP
The LinearSVC class regularizes the bias term, so you should center the
training set first by subtracting its mean. This is automatic if you scale the
data using the StandardScaler. Also make sure you set
the loss hyperparameter to "hinge", as it is not the default value. Finally, for
better performance, you should set the dual hyperparameter to False,
unless there are more features than training instances (we will discuss
duality later in the chapter).


```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

svm_clf = Pipeline([
 #("scaler", StandardScaler()),
 ("linear_svc", LinearSVC(max_iter=5000)),
 ])
svm_clf.fit(X_train, y_train)  
```




    Pipeline(memory=None,
             steps=[('linear_svc',
                     LinearSVC(C=1.0, class_weight=None, dual=True,
                               fit_intercept=True, intercept_scaling=1,
                               loss='squared_hinge', max_iter=5000,
                               multi_class='ovr', penalty='l2', random_state=None,
                               tol=0.0001, verbose=0))],
             verbose=False)




```python
svm_predict = svm_clf.predict(X_test)
print( classification_report(y_test, svm_predict) )

print( accuracy_score(y_test, svm_predict))
```

                  precision    recall  f1-score   support

               0       1.00      1.00      1.00        11
               1       1.00      1.00      1.00        13
               2       1.00      1.00      1.00         6

        accuracy                           1.00        30
       macro avg       1.00      1.00      1.00        30
    weighted avg       1.00      1.00      1.00        30

    1.0



```python
svm_matrix = metrics.confusion_matrix(y_test, svm_predict)
svm_matrix
```




    array([[11,  0,  0],
           [ 0, 13,  0],
           [ 0,  0,  6]], dtype=int64)



### 3.3 Decision Trees
Like SVMs, Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks. They are powerful algorithms, capable of fitting complex datasets.  
To understand Decision Trees, let’s build one and take a look at how it
makes predictions. The following code trains a DecisionTreeClassifier on
the iris dataset (see Chapter 4):
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
iris = load_iris()
X = iris.data[:, 2:] # petal length and width
y = iris.target
tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X, y).  

You can visualize the trained Decision Tree by first using
the export_graphviz() method to output a graph definition file
called iris_tree.dot:
from sklearn.tree import export_graphviz
export_graphviz(
 tree_clf,
 out_file=image_path("iris_tree.dot"),
 feature_names=iris.feature_names[2:],
 class_names=iris.target_names,
 rounded=True,
 filled=True
 ).  
 Then you can use the dot command-line tool from the Graphviz package to
convert this .dot file to a variety of formats, such as PDF or PNG.1 This
command line converts the .dot file to a .png image file:
$ dot -Tpng iris_tree.dot -o iris_tree.png .  
Making Predictions
Let’s see how the tree represented in Figure 6-1 makes predictions.
Suppose you find an iris flower and you want to classify it. You start at
the root node (depth 0, at the top): this node asks whether the flower’s
petal length is smaller than 2.45 cm. If it is, then you move down to the
root’s left child node (depth 1, left). In this case, it is a leaf node (i.e., it does
not have any child nodes), so it does not ask any questions: simply look at
the predicted class for that node, and the Decision Tree predicts that your
flower is an Iris setosa (class=setosa).
Now suppose you find another flower, and this time the petal length is
greater than 2.45 cm. You must move down to the root’s right child node
(depth 1, right), which is not a leaf node, so the node asks another question:
is the petal width smaller than 1.75 cm? If it is, then your flower is most
likely an Iris versicolor (depth 2, left). If not, it is likely an Iris
virginica (depth 2, right). It’s really that simple.  
A node’s samples attribute counts how many training instances it applies to.
For example, 100 training instances have a petal length greater than 2.45
cm (depth 1, right), and of those 100, 54 have a petal width smaller than
1.75 cm (depth 2, left). A node’s value attribute tells you how many training
instances of each class this node applies to: for example, the bottom-right
node applies to 0 Iris setosa, 1 Iris versicolor, and 45 Iris virginica. Finally, a
node’s gini attribute measures its impurity: a node is “pure” (gini=0) if all
training instances it applies to belong to the same class. For example, since
the depth-1 left node applies only to Iris setosa training instances, it is pure
and its gini score is 0. Equation 6-1 shows how the training algorithm
computes the gini score Gi of the i
th node. The depth-2 left node has
a gini score equal to 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168.
Equation 6-1. Gini impurity
Gi=1−∑k=1npi,k2Gi=1-∑k=1npi,k2
In this equation:
• pi,k is the ratio of class k instances among the training instances in
the i
th node.
NOTE
Scikit-Learn uses the CART algorithm, which produces only binary trees:
nonleaf nodes always have two children (i.e., questions only have yes/no
answers). However, other algorithms such as ID3 can produce Decision
Trees with nodes that have more than two children.  
Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to
train Decision Trees (also called “growing” trees). The algorithm works by
first splitting the training set into two subsets using a single feature k and a
threshold tk (e.g., “petal length ≤ 2.45 cm”). How does it choose k and tk? It
searches for the pair (k, tk) that produces the purest subsets (weighted by
their size). Equation 6-2 gives the cost function that the algorithm tries to
minimize.  
As you can see, the CART algorithm is a greedy algorithm: it greedily
searches for an optimum split at the top level, then repeats the process at
each subsequent level. It does not check whether or not the split will lead
to the lowest possible impurity several levels down. A greedy algorithm
often produces a solution that’s reasonably good but not guaranteed to be
optimal.
Unfortunately, finding the optimal tree is known to be an NPComplete problem:2 it requires O(exp(m)) time, making the problem
intractable even for small training sets. This is why we must settle for a
“reasonably good” solution.  
To avoid overfitting the training data, you need to restrict the Decision
Tree’s freedom during training. As you know by now, this is called
regularization. The regularization hyperparameters depend on the
algorithm used, but generally you can at least restrict the maximum depth
of the Decision Tree. In Scikit-Learn, this is controlled by
the max_depth hyperparameter (the default value is None, which means
unlimited). Reducing max_depth will regularize the model and thus reduce
the risk of overfitting.
The DecisionTreeClassifier class has a few other parameters that similarly
restrict the shape of the Decision Tree: min_samples_split (the minimum
number of samples a node must have before it can be
split), min_samples_leaf (the minimum number of samples a leaf node must
have), min_weight_fraction_leaf (same as min_samples_leaf but expressed as
a fraction of the total number of weighted instances), max_leaf_nodes (the
maximum number of leaf nodes), and max_features (the maximum number
of features that are evaluated for splitting at each node).
Increasing min_* hyperparameters or reducing max_* hyperparameters will
regularize the model.  

### 3.4. Ensemble Methods
Suppose you pose a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method.  
Let's take a real example to build the intuition.

Suppose, you want to invest in a company XYZ. You are not sure about its performance though. So, you look for advice on whether the stock price will increase by more than 6% per annum or not? You decide to approach various experts having diverse domain experience.  
An ensemble is the art of combining a diverse set of learners (individual models) together to improvise on the stability and predictive power of the model. In the above example, the way we combine all the predictions collectively will be termed as Ensemble learning.  
As an example of an Ensemble method, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you obtain the predictions of all the individual trees, then predict the class that gets the most votes (see the last exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest, and despite its
simplicity, this is one of the most powerful Machine Learning algorithms available today.

### 3.4.1. Voting Classifiers
Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1).  
A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier (see Figure 7-.  
Similarly, suppose you build an ensemble containing 1,000 classifiers that are individually correct only 51% of the time (barely better than random guessing). If you predict the majority voted class, you can hope for up to 75% accuracy! However, this is only true if all classifiers are perfectly independent, making uncorrelated errors, which is clearly not the case because they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy. TIP Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them
using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.  
The following code creates and trains a voting classifier in Scikit-Learn,
composed of three diverse classifiers (the training set is the moons dataset,
introduced in Chapter 5):
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()
voting_clf = VotingClassifier(
 estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
 voting='hard')
voting_clf.fit(X_train, y_train)
Let’s look at each classifier’s accuracy on the test set:
>>> from sklearn.metrics import accuracy_score
>>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
... clf.fit(X_train, y_train)
... y_pred = clf.predict(X_test)
... print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
...
LogisticRegression 0.864
RandomForestClassifier 0.896
SVC 0.888
VotingClassifier 0.904

### 3.4.2. Bagging Classifiers
One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows
training instances to be sampled several times for the same predictor. This sampling and training process is represented in Figure 7-4.  
Bagging is one of the Ensemble construction techniques which is also known as Bootstrap Aggregation. Bootstrap establishes the foundation of Bagging technique. Bootstrap is a sampling technique in which we select “n” observations out of a population of “n” observations. But the selection is entirely random, i.e., each observation can be chosen from the original population so that each observation is equally likely to be selected in each iteration of the bootstrapping process. After the bootstrapped samples are formed, separate models are trained with the bootstrapped samples. In real experiments, the bootstrapped samples are drawn from the training set, and the sub-models are tested using the testing set. The final output prediction is combined across the projections of all the sub-models.  
<img src= 'bagging.png'> [Source](https://hudsonthames.org/bagging-in-financial-machine-learning-sequential-bootstrapping-python/)
Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it
were trained on the original training set, but aggregation reduces both bias and variance.4 Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.
Bagging and Pasting in Scikit-Learn
Scikit-Learn offers a simple API for both bagging and pasting with
the BaggingClassifier class (or BaggingRegressor for regression). The
following code trains an ensemble of 500 Decision Tree classifiers:5 each is
trained on 100 training instances randomly sampled from the training set
with replacement (this is an example of bagging, but if you want to use
pasting instead, just set bootstrap=False). The n_jobs parameter tells ScikitLearn the number of CPU cores to use for training and predictions (–1 tells
Scikit-Learn to use all available cores):
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
bag_clf = BaggingClassifier(
 DecisionTreeClassifier(), n_estimators=500,
 max_samples=100, bootstrap=True, n_jobs=-1)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
NOTE
The BaggingClassifier automatically performs soft voting instead of hard
voting if the base classifier can estimate class probabilities (i.e., if it has
a predict_proba() method), which is the case with Decision Tree classifiers.

### 3.4.3. Random Forests
As we have discussed, a Random Forest9 is an ensemble of Decision Trees,
generally trained via the bagging method (or sometimes pasting), typically
with max_samples set to the size of the training set. Instead of building
a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead
use the RandomForestClassifier class, which is more convenient and
optimized for Decision Trees10 (similarly, there is
a RandomForestRegressor class for regression tasks). The following code uses
all available CPU cores to train a Random Forest classifier with 500 trees
(each limited to maximum 16 nodes):
from sklearn.ensemble import RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16,
n_jobs=-1)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
With a few exceptions, a RandomForestClassifier has all the hyperparameters
of a DecisionTreeClassifier (to control how trees are grown), plus all the
hyperparameters of a BaggingClassifier to control the ensemble itself.11
The Random Forest algorithm introduces extra randomness when growing
trees; instead of searching for the very best feature when splitting a node
(see Chapter 6), it searches for the best feature among a random subset of
features. The algorithm results in greater tree diversity, which (again)
trades a higher bias for a lower variance, generally yielding an overall
better model. The following BaggingClassifier is roughly equivalent to the
previous RandomForestClassifier:
bag_clf = BaggingClassifier(
 DecisionTreeClassifier(splitter="random", max_leaf_nodes=16),
 n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)
Extra-Trees
When you are growing a tree in a Random Forest, at each node only a
random subset of the features is considered for splitting (as discussed
earlier). It is possible to make trees even more random by also using
random thresholds for each feature rather than searching for the best
possible thresholds (like regular Decision Trees do).
A forest of such extremely random trees is called an Extremely Randomized
Trees ensemble12 (or Extra-Trees for short). Once again, this technique
trades more bias for a lower variance. It also makes Extra-Trees much
faster to train than regular Random Forests, because finding the best
possible threshold for each feature at every node is one of the most timeconsuming tasks of growing a tree.
You can create an Extra-Trees classifier using ScikitLearn’s ExtraTreesClassifier class. Its API is identical to
the RandomForestClassifier class. Similarly, the ExtraTreesRegressor class has
the same API as the RandomForestRegressor class.
TIP
It is hard to tell in advance whether a RandomForestClassifier will perform
better or worse than an ExtraTreesClassifier. Generally, the only way to
know is to try both and compare them using cross-validation (tuning the
hyperparameters using grid search).
Feature Importance
Yet another great quality of Random Forests is that they make it easy to
measure the relative importance of each feature. Scikit-Learn measures a
feature’s importance by looking at how much the tree nodes that use that
feature reduce impurity on average (across all trees in the forest). More
precisely, it is a weighted average, where each node’s weight is equal to the
number of training samples that are associated with it (see Chapter 6).
Scikit-Learn computes this score automatically for each feature after
training, then it scales the results so that the sum of all importances is
equal to 1. You can access the result using the feature_importances_ variable.
For example, the following code trains a RandomForestClassifier on the iris
dataset (introduced in Chapter 4) and outputs each feature’s importance. It
seems that the most important features are the petal length (44%) and width (42%), while sepal length and width are rather unimportant in
comparison (11% and 2%, respectively):
>>> from sklearn.datasets import load_iris
>>> iris = load_iris()
>>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
>>> rnd_clf.fit(iris["data"], iris["target"])
>>> for name, score in zip(iris["feature_names"],
rnd_clf.feature_importances_):
... print(name, score)
...
sepal length (cm) 0.112492250999
sepal width (cm) 0.0231192882825
petal length (cm) 0.441030464364
petal width (cm) 0.423357996355


### 3.5. Boosting
Boosting (originally called hypothesis boosting) refers to any Ensemble
method that can combine several weak learners into a strong learner. The
general idea of most boosting methods is to train predictors sequentially,
each trying to correct its predecessor. There are many boosting methods
available, but by far the most popular are AdaBoost13 (short for Adaptive
Boosting) and Gradient Boosting. Let’s start with AdaBoost.  


### 3.5.1. AdaBoost
One way for a new predictor to correct its predecessor is to pay a bit more
attention to the training instances that the predecessor underfitted. This
results in new predictors focusing more and more on the hard cases. This is
the technique used by AdaBoost.
For example, when training an AdaBoost classifier, the algorithm first
trains a base classifier (such as a Decision Tree) and uses it to make
predictions on the training set. The algorithm then increases the relative
weight of misclassified training instances. Then it trains a second classifier,
using the updated weights, and again makes predictions on the training set,
updates the instance weights, and so on (see Figure 7-7).  
Let’s take a closer look at the AdaBoost algorithm. Each instance
weight w(
i
) is initially set to 1/m. A first predictor is trained, and its
weighted error rate r1 is computed on the training set; see Equation 7-1.  
The predictor’s weight αj is then computed using Equation 7-2, where η is
the learning rate hyperparameter (defaults to 1).15 The more accurate the
predictor is, the higher its weight will be. If it is just guessing randomly,
then its weight will be close to zero. However, if it is most often wrong (i.e.,
less accurate than random guessing), then its weight will be negative.  
Next, the AdaBoost algorithm updates the instance weights, using Equation
7-3, which boosts the weights of the misclassified instances.  
Finally, a new predictor is trained using the updated weights, and the
whole process is repeated (the new predictor’s weight is computed, the
instance weights are updated, then another predictor is trained, and so on).
The algorithm stops when the desired number of predictors is reached, or
when a perfect predictor is found.
To make predictions, AdaBoost simply computes the predictions of all the
predictors and weighs them using the predictor weights αj. The predicted
class is the one that receives the majority of weighted votes (see Equation.  
Scikit-Learn uses a multiclass version of AdaBoost called SAMME16 (which
stands for Stagewise Additive Modeling using a Multiclass Exponential loss
function). When there are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate class probabilities (i.e., if they have
a predict_proba() method), Scikit-Learn can use a variant of SAMME
called SAMME.R (the R stands for “Real”), which relies on class probabilities
rather than predictions and generally performs better.
The following code trains an AdaBoost classifier based on 200 Decision
Stumps using Scikit-Learn’s AdaBoostClassifier class (as you might expect,
there is also an AdaBoostRegressor class). A Decision Stump is a Decision Tree
with max_depth=1—in other words, a tree composed of a single decision
node plus two leaf nodes. This is the default base estimator for
the AdaBoostClassifier class:
from sklearn.ensemble import AdaBoostClassifier
ada_clf = AdaBoostClassifier(
 DecisionTreeClassifier(max_depth=1), n_estimators=200,
 algorithm="SAMME.R", learning_rate=0.5)
ada_clf.fit(X_train, y_train)
TIP
If your AdaBoost ensemble is overfitting the training set, you can try
reducing the number of estimators or more strongly regularizing the base
estimator.


### Gradient Boosting
Another very popular boosting algorithm is Gradient Boosting.17 Just like
AdaBoost, Gradient Boosting works by sequentially adding predictors to an
ensemble, each one correcting its predecessor. However, instead of
tweaking the instance weights at every iteration like AdaBoost does, this
method tries to fit the new predictor to the residual errors made by the
previous predictor.
Let’s go through a simple regression example, using Decision Trees as the
base predictors (of course, Gradient Boosting also works great with
regression tasks). This is called Gradient Tree Boosting, or Gradient Boosted
Regression Trees (GBRT). First, let’s fit a DecisionTreeRegressor to the
training set (for example, a noisy quadratic training set).  
from sklearn.tree import DecisionTreeRegressor
tree_reg1 = DecisionTreeRegressor(max_depth=2)
tree_reg1.fit(X, y)
Next, we’ll train a second DecisionTreeRegressor on the residual errors made
by the first predictor:
y2 = y - tree_reg1.predict(X)
tree_reg2 = DecisionTreeRegressor(max_depth=2)
tree_reg2.fit(X, y2)
Then we train a third regressor on the residual errors made by the second
predictor:
y3 = y2 - tree_reg2.predict(X)
tree_reg3 = DecisionTreeRegressor(max_depth=2)
tree_reg3.fit(X, y3)
Now we have an ensemble containing three trees. It can make predictions
on a new instance simply by adding up the predictions of all the trees:
y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2,
tree_reg3))
Figure 7-9 represents the predictions of these three trees in the left
column, and the ensemble’s predictions in the right column. In the first
row, the ensemble has just one tree, so its predictions are exactly the same
as the first tree’s predictions. In the second row, a new tree is trained on
the residual errors of the first tree. On the right you can see that the
ensemble’s predictions are equal to the sum of the predictions of the first
two trees. Similarly, in the third row another tree is trained on the residual
errors of the second tree. You can see that the ensemble’s predictions
gradually get better as trees are added to the ensemble.
A simpler way to train GBRT ensembles is to use ScikitLearn’s GradientBoostingRegressor class. Much like
the RandomForestRegressor class, it has hyperparameters to control the
growth of Decision Trees (e.g., max_depth, min_samples_leaf), as well as
hyperparameters to control the ensemble training, such as the number of
trees (n_estimators). The following code creates the same ensemble as the
previous one


```python

```
