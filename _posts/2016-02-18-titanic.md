---
title: "Kaggle Prediction Exercise: Titanic"
category: ML
tags: [Data Science]
date: 2018-06-18
header:
  image: "/images/ml1.jpg"
excerpt: "Data Science, Supervised Learning"
---

## 1. Import libraries and data


```python
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
sns.set_style('whitegrid')
sns.set(rc = {'figure.figsize':(15, 10)})

import warnings
warnings.filterwarnings(action="ignore", message="internal gelsd")
warnings.simplefilter(action='ignore', category=FutureWarning)

```


```python
train_df = pd.read_csv('train.csv')
```


```python
test_df = pd.read_csv('test.csv')
```


```python
submission_df = pd.read_csv('gender_submission.csv')
```

## 2. Some data analysis


```python
print(train_df.shape)
print(test_df.shape)
```

    (891, 12)
    (418, 11)



```python
train_df['Survived'].value_counts()
```




    0    549
    1    342
    Name: Survived, dtype: int64




```python
train_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.options.display.max_columns
train_df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>714.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>446.000000</td>
      <td>0.383838</td>
      <td>2.308642</td>
      <td>29.699118</td>
      <td>0.523008</td>
      <td>0.381594</td>
      <td>32.204208</td>
    </tr>
    <tr>
      <th>std</th>
      <td>257.353842</td>
      <td>0.486592</td>
      <td>0.836071</td>
      <td>14.526497</td>
      <td>1.102743</td>
      <td>0.806057</td>
      <td>49.693429</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>223.500000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.125000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.910400</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>446.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>668.500000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>891.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
    </tr>
  </tbody>
</table>
</div>




```python
train_df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object
     4   Sex          891 non-null    object
     5   Age          714 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object
     9   Fare         891 non-null    float64
     10  Cabin        204 non-null    object
     11  Embarked     889 non-null    object
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB


## 3. Feature Engineering
This is involves the process of getting the most out of data provided.

### 3.1. Feature Extractions

#### *Create Title from Name feature


```python
#First, we add both train and test set together for this chapter alone.
data = pd.concat([train_df, test_df])
data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 1309 entries, 0 to 417
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  1309 non-null   int64  
     1   Survived     891 non-null    float64
     2   Pclass       1309 non-null   int64  
     3   Name         1309 non-null   object
     4   Sex          1309 non-null   object
     5   Age          1046 non-null   float64
     6   SibSp        1309 non-null   int64  
     7   Parch        1309 non-null   int64  
     8   Ticket       1309 non-null   object
     9   Fare         1308 non-null   float64
     10  Cabin        295 non-null    object
     11  Embarked     1307 non-null   object
    dtypes: float64(3), int64(4), object(5)
    memory usage: 132.9+ KB



```python
import re
data['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\.', x).group(1))
data.Title.value_counts()
```




    Mr          757
    Miss        260
    Mrs         197
    Master       61
    Rev           8
    Dr            8
    Col           4
    Ms            2
    Major         2
    Mlle          2
    Dona          1
    Mme           1
    Capt          1
    Countess      1
    Lady          1
    Don           1
    Sir           1
    Jonkheer      1
    Name: Title, dtype: int64




```python
#We can replace these titles with only 6 titles (Mr, Mrs, Miss, Master, other, Military and Nobility)
data['Title'] = data['Title'].replace(
{
    'Mile':'Miss', 'Ms':'Miss', 'Mlle':'Miss',
    'Mme':'Mrs', 'Dona':'Mrs',
    'Don':'Mr',
    'Jonkheer':'Nobility', 'Lady':'Nobility', 'Sir':'Nobility', 'Countess':'Nobility',
    'Capt':'Military', 'Major':'Military', 'Col':'Military',
    'Rev':'Other', 'Dr':'Other'
})
data['Title'].value_counts()
```




    Mr          758
    Miss        264
    Mrs         199
    Master       61
    Other        16
    Military      7
    Nobility      4
    Name: Title, dtype: int64



#### *Create Family  from Sibsp and Parch


```python
data['Family'] = data['SibSp'] + data['Parch'] + 1
```

#### *Edit Ticket


```python
data['Ticket_1'] = data['Ticket'].map(lambda x: re.sub('\D', '', x))
data['Ticket_1'] = pd.to_numeric(data['Ticket_1'])

ticket=dict(data['Ticket'].value_counts())
data['Ticket_2'] = data['Ticket'].map(ticket)
```

#### *Binning Fare and Age


```python
data['Bin_Fare'] = pd.qcut(data.Fare, q=4, labels=False)
data['Bin_Age'] = pd.qcut(data.Age, q=10, labels=False)
```

#### *Create Deck and Has_Cabin from Cabin


```python
data.sample(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>Family</th>
      <th>Ticket_1</th>
      <th>Ticket_2</th>
      <th>Bin_Fare</th>
      <th>Bin_Age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>684</th>
      <td>685</td>
      <td>0.0</td>
      <td>2</td>
      <td>Brown, Mr. Thomas William Solomon</td>
      <td>male</td>
      <td>60.0</td>
      <td>1</td>
      <td>1</td>
      <td>29750</td>
      <td>39.0000</td>
      <td>NaN</td>
      <td>S</td>
      <td>Mr</td>
      <td>3</td>
      <td>29750.0</td>
      <td>3</td>
      <td>3.0</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>348</th>
      <td>349</td>
      <td>1.0</td>
      <td>3</td>
      <td>Coutts, Master. William Loch "William"</td>
      <td>male</td>
      <td>3.0</td>
      <td>1</td>
      <td>1</td>
      <td>C.A. 37671</td>
      <td>15.9000</td>
      <td>NaN</td>
      <td>S</td>
      <td>Master</td>
      <td>3</td>
      <td>37671.0</td>
      <td>3</td>
      <td>2.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>624</th>
      <td>625</td>
      <td>0.0</td>
      <td>3</td>
      <td>Bowen, Mr. David John "Dai"</td>
      <td>male</td>
      <td>21.0</td>
      <td>0</td>
      <td>0</td>
      <td>54636</td>
      <td>16.1000</td>
      <td>NaN</td>
      <td>S</td>
      <td>Mr</td>
      <td>1</td>
      <td>54636.0</td>
      <td>2</td>
      <td>2.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>339</th>
      <td>1231</td>
      <td>NaN</td>
      <td>3</td>
      <td>Betros, Master. Seman</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>2622</td>
      <td>7.2292</td>
      <td>NaN</td>
      <td>C</td>
      <td>Master</td>
      <td>1</td>
      <td>2622.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>232</th>
      <td>233</td>
      <td>0.0</td>
      <td>2</td>
      <td>Sjostedt, Mr. Ernst Adolf</td>
      <td>male</td>
      <td>59.0</td>
      <td>0</td>
      <td>0</td>
      <td>237442</td>
      <td>13.5000</td>
      <td>NaN</td>
      <td>S</td>
      <td>Mr</td>
      <td>1</td>
      <td>237442.0</td>
      <td>1</td>
      <td>1.0</td>
      <td>9.0</td>
    </tr>
  </tbody>
</table>
</div>



### 3.3. Drop Features


```python
data.columns
```




    Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',
           'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Title', 'Family',
           'Ticket_1', 'Ticket_2', 'Bin_Fare', 'Bin_Age'],
          dtype='object')




```python
#drop features you suspect are unimportant
data = data.drop(['Name', 'PassengerId', 'Survived', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1)
#drop after feature_importance
data = data.drop(['Bin_Fare'], axis=1)
```

### 3.4. Feature Transform


```python
#separate dataset back to train and test set
train = data.iloc[:891]
test = data.iloc[891:]
train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>Family</th>
      <th>Ticket_1</th>
      <th>Ticket_2</th>
      <th>Bin_Age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>7.2500</td>
      <td>S</td>
      <td>Mr</td>
      <td>2</td>
      <td>521171.0</td>
      <td>1</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>71.2833</td>
      <td>C</td>
      <td>Mrs</td>
      <td>2</td>
      <td>17599.0</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>7.9250</td>
      <td>S</td>
      <td>Miss</td>
      <td>1</td>
      <td>23101282.0</td>
      <td>1</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>53.1000</td>
      <td>S</td>
      <td>Mrs</td>
      <td>2</td>
      <td>113803.0</td>
      <td>2</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>8.0500</td>
      <td>S</td>
      <td>Mr</td>
      <td>1</td>
      <td>373450.0</td>
      <td>1</td>
      <td>6.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
#create a sample of numerical predictors
train_num = train.select_dtypes(include=[np.number])
train_num.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Age</th>
      <th>Fare</th>
      <th>Family</th>
      <th>Ticket_1</th>
      <th>Ticket_2</th>
      <th>Bin_Age</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>22.0</td>
      <td>7.2500</td>
      <td>2</td>
      <td>521171.0</td>
      <td>1</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>38.0</td>
      <td>71.2833</td>
      <td>2</td>
      <td>17599.0</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>26.0</td>
      <td>7.9250</td>
      <td>1</td>
      <td>23101282.0</td>
      <td>1</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>35.0</td>
      <td>53.1000</td>
      <td>2</td>
      <td>113803.0</td>
      <td>2</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>35.0</td>
      <td>8.0500</td>
      <td>1</td>
      <td>373450.0</td>
      <td>1</td>
      <td>6.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# I am going to create a numeric pipeline where I would impute the median to missing numbers and then scale the datapoints
from sklearn.pipeline import Pipeline as pl
from sklearn.impute import SimpleImputer as si
from sklearn.preprocessing import RobustScaler

num_pipeline = pl([
    ('imputer', si(strategy='median')),
    ('scaler', RobustScaler()),
])

train_num_trx = num_pipeline.fit_transform(train_num)
train_num_trx
```




    array([[ 0.00000000e+00, -4.61538462e-01, -3.12010602e-01, ...,
             1.23510357e+00,  0.00000000e+00, -5.00000000e-01],
           [-2.00000000e+00,  7.69230769e-01,  2.46124229e+00, ...,
            -2.90816698e-01,  5.00000000e-01,  7.50000000e-01],
           [ 0.00000000e+00, -1.53846154e-01, -2.82776661e-01, ...,
             6.96571943e+01,  0.00000000e+00,  0.00000000e+00],
           ...,
           [ 0.00000000e+00,  0.00000000e+00,  3.89603978e-01, ...,
            -3.24124577e-01,  1.50000000e+00,  0.00000000e+00],
           [-2.00000000e+00, -1.53846154e-01,  6.73281477e-01, ...,
            -6.67551483e-03,  0.00000000e+00,  0.00000000e+00],
           [ 0.00000000e+00,  3.07692308e-01, -2.90355831e-01, ...,
             7.78165642e-01,  0.00000000e+00,  5.00000000e-01]])




```python
#create dataframe of object type  
train_cat = train.select_dtypes(exclude=[np.number])
train_cat.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sex</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>male</td>
      <td>S</td>
      <td>Mr</td>
    </tr>
    <tr>
      <th>1</th>
      <td>female</td>
      <td>C</td>
      <td>Mrs</td>
    </tr>
    <tr>
      <th>2</th>
      <td>female</td>
      <td>S</td>
      <td>Miss</td>
    </tr>
    <tr>
      <th>3</th>
      <td>female</td>
      <td>S</td>
      <td>Mrs</td>
    </tr>
    <tr>
      <th>4</th>
      <td>male</td>
      <td>S</td>
      <td>Mr</td>
    </tr>
  </tbody>
</table>
</div>




```python
# convert object type predictors to numeric
from sklearn.preprocessing import OneHotEncoder as onehot

cat_pipeline = pl([
    ('imputer', si(strategy='most_frequent')),
    ('encoder', onehot(sparse=False, drop='first')),
])
train_cat_trx = cat_pipeline.fit_transform(train_cat)
train_cat_trx
```




    array([[1., 0., 1., ..., 0., 0., 0.],
           [0., 0., 0., ..., 1., 0., 0.],
           [0., 0., 1., ..., 0., 0., 0.],
           ...,
           [0., 0., 1., ..., 0., 0., 0.],
           [1., 0., 0., ..., 0., 0., 0.],
           [1., 1., 0., ..., 0., 0., 0.]])




```python
#Combine both pipelines
from sklearn.compose import ColumnTransformer as colt

num_attribs = list(train_num)
cat_attribs = list(train_cat)

full_pipeline = colt([
    ('num', num_pipeline, num_attribs),
    ('cat', cat_pipeline, cat_attribs),
])
```

### 3.5. Recheck and Rename


```python
print(train_cat_trx.shape)
print(train_num_trx.shape)
```

    (891, 9)
    (891, 7)



```python
X_train = full_pipeline.fit_transform(train)
```


```python
X_test = full_pipeline.transform(test)
```


```python
y_train = train_df['Survived']
```


```python
y_test = submission_df['Survived']
```

## 4. Modelling
Adaboost decisiontree, random_forest and gradientboosting


```python
from sklearn.ensemble import RandomForestClassifier as rfc


forest = rfc(random_state=0, n_estimators=500, max_leaf_nodes=16)
forest.fit(X_train, y_train)
forest_ts = forest.predict(X_test)
forest_ts
```




    array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
           1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
           1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
           1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
           1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
           0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
           0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
           1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
           0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
           1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
           0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
           0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
           0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
           0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
           1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
           0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
           1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
           0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1],
          dtype=int64)




```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')
lda.fit(X_train, y_train)
lda_ts = lda.predict(X_test)
```


```python
from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(random_state=0, n_estimators=500)
gbc.fit(X_train, y_train)
gbc_ts = gbc.predict(X_test)
```


```python
from sklearn.linear_model import LogisticRegression

log = LogisticRegression(max_iter=500, random_state=0)
log.fit(X_train, y_train)
log_ts = log.predict(X_test)
```


```python
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier

svc = BaggingClassifier(
    SVC(C=0.7, gamma='auto', random_state=0), random_state=0, n_estimators=500)

svc.fit(X_train, y_train)
svc_ts = svc.predict(X_test)
```


```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(
     LogisticRegression(max_iter=500, random_state=0)
)
ada.fit(X_train, y_train)
```




    AdaBoostClassifier(algorithm='SAMME.R',
                       base_estimator=LogisticRegression(C=1.0, class_weight=None,
                                                         dual=False,
                                                         fit_intercept=True,
                                                         intercept_scaling=1,
                                                         l1_ratio=None,
                                                         max_iter=500,
                                                         multi_class='auto',
                                                         n_jobs=None, penalty='l2',
                                                         random_state=0,
                                                         solver='lbfgs', tol=0.0001,
                                                         verbose=0,
                                                         warm_start=False),
                       learning_rate=1.0, n_estimators=50, random_state=None)



#### let us see how they all performed


```python
from sklearn.metrics import accuracy_score

for clf in (forest, lda, gbc, log, svc, ada):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_pred, y_test))
```

    RandomForestClassifier 0.9401913875598086
    LinearDiscriminantAnalysis 0.9736842105263158
    GradientBoostingClassifier 0.8301435406698564
    LogisticRegression 0.937799043062201
    BaggingClassifier 0.9425837320574163
    AdaBoostClassifier 0.9449760765550239


***They are all overfitting except gradient boost classifier. Who can tell me why?***

## 5. Evaluation
 Lets see how our model would perform without overfitting to the data


```python
from sklearn.ensemble import VotingClassifier

voting_clf = VotingClassifier(
    estimators=[('rfc',forest), ('svc', svc), ('gbc', gbc), ('ada', ada)], voting='hard')

voting_clf.fit(X_train, y_train)
```




    VotingClassifier(estimators=[('rfc',
                                  RandomForestClassifier(bootstrap=True,
                                                         ccp_alpha=0.0,
                                                         class_weight=None,
                                                         criterion='gini',
                                                         max_depth=None,
                                                         max_features='auto',
                                                         max_leaf_nodes=16,
                                                         max_samples=None,
                                                         min_impurity_decrease=0.0,
                                                         min_impurity_split=None,
                                                         min_samples_leaf=1,
                                                         min_samples_split=2,
                                                         min_weight_fraction_leaf=0.0,
                                                         n_estimators=500,
                                                         n_jobs=None,
                                                         oob_score=...
                                                     base_estimator=LogisticRegression(C=1.0,
                                                                                       class_weight=None,
                                                                                       dual=False,
                                                                                       fit_intercept=True,
                                                                                       intercept_scaling=1,
                                                                                       l1_ratio=None,
                                                                                       max_iter=500,
                                                                                       multi_class='auto',
                                                                                       n_jobs=None,
                                                                                       penalty='l2',
                                                                                       random_state=0,
                                                                                       solver='lbfgs',
                                                                                       tol=0.0001,
                                                                                       verbose=0,
                                                                                       warm_start=False),
                                                     learning_rate=1.0,
                                                     n_estimators=50,
                                                     random_state=None))],
                     flatten_transform=True, n_jobs=None, voting='hard',
                     weights=None)




```python
#lets compare our models
from sklearn.model_selection import cross_val_score

forest_scores = cross_val_score(forest,  X_train, y_train, cv=5)
print(forest_scores.mean())

lda_scores = cross_val_score(lda, X_train, y_train, cv=5)
print(lda_scores.mean())

gbc_scores = cross_val_score(gbc, X_train, y_train, cv=5)
print(gbc_scores.mean())

log_scores = cross_val_score(log,  X_train, y_train, cv=5)
print(log_scores.mean())

svc_scores = cross_val_score(svc,  X_train, y_train, cv=5)
print(svc_scores.mean())

ada_scores = cross_val_score(ada,  X_train, y_train, cv=5)
print(ada_scores.mean())

vot_scores = cross_val_score(voting_clf,  X_train, y_train, cv=5)
print(vot_scores.mean())
```

    0.8248948590797817
    0.8091959073504489
    0.8383654510074697
    0.8215366267026551
    0.8249199673592367
    0.8170547988199109
    0.8282719226664993


### 5.1. Feature Importance


```python
names = list(train)
print ("Features sorted by their score:")
print (sorted(zip(map(lambda x: round(x, 4), forest.feature_importances_), names),
             reverse=True))
```

    Features sorted by their score:
    [(0.2211, 'Ticket_1'), (0.0902, 'Pclass'), (0.084, 'Age'), (0.0764, 'Embarked'), (0.0561, 'Title'), (0.0446, 'Fare'), (0.0439, 'Sex'), (0.0226, 'Family'), (0.0086, 'Bin_Age'), (0.0044, 'Ticket_2')]


## 6. Submision
Select best achieving model and submit


```python
y_pred = voting_clf.predict(X_test)
```


```python
output = pd.DataFrame({'PassengerId':submission_df.PassengerId, 'Survived': y_pred})
```


```python
output.to_csv('vote4.csv', index=False)
```


```python
output.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>895</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>896</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



Find ways to get a higher score. This was my rank at kaggle.

<img src="{{ site.url }}{{ site.baseurl }}/images/titanic.png">


```python

```
