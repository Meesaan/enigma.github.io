---
title: "Kaggle Exercise: Titanic"
category: ML
tags: [Data Science]
date: 2018-02-18
header:
  image: "/images/ml1.jpg"
excerpt: "Data Science, Supervised Learning"
---

## 1. Import libraries and data


```python
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
sns.set_style('whitegrid')
sns.set(rc = {'figure.figsize':(15, 10)})

import warnings
warnings.filterwarnings(action="ignore", message="internal gelsd")
warnings.simplefilter(action='ignore', category=FutureWarning)

```


```python
train_df = pd.read_csv('train.csv')
```


```python
test_df = pd.read_csv('test.csv')
```


```python
submission_df = pd.read_csv('gender_submission.csv')
```

## 2. Some data analysis


```python
print(train_df.shape)
print(test_df.shape)
```

    (891, 12)
    (418, 11)



```python
train_df['Survived'].value_counts()
```




    0    549
    1    342
    Name: Survived, dtype: int64




```python
train_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <td>4</td>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




```python
pd.options.display.max_columns
train_df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>count</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>714.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <td>mean</td>
      <td>446.000000</td>
      <td>0.383838</td>
      <td>2.308642</td>
      <td>29.699118</td>
      <td>0.523008</td>
      <td>0.381594</td>
      <td>32.204208</td>
    </tr>
    <tr>
      <td>std</td>
      <td>257.353842</td>
      <td>0.486592</td>
      <td>0.836071</td>
      <td>14.526497</td>
      <td>1.102743</td>
      <td>0.806057</td>
      <td>49.693429</td>
    </tr>
    <tr>
      <td>min</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>223.500000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.125000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.910400</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>446.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>668.500000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <td>max</td>
      <td>891.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
    </tr>
  </tbody>
</table>
</div>




```python
train_df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
    PassengerId    891 non-null int64
    Survived       891 non-null int64
    Pclass         891 non-null int64
    Name           891 non-null object
    Sex            891 non-null object
    Age            714 non-null float64
    SibSp          891 non-null int64
    Parch          891 non-null int64
    Ticket         891 non-null object
    Fare           891 non-null float64
    Cabin          204 non-null object
    Embarked       889 non-null object
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB


## 3. Feature Engineering
This is involves the process of getting the most out of data provided.

### 3.1. Feature Extractions

#### Create Title from Name feature


```python
#First, we add both train and test set together for this chapter alone.
data = pd.concat([train_df, test_df])
data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 1309 entries, 0 to 417
    Data columns (total 12 columns):
    Age            1046 non-null float64
    Cabin          295 non-null object
    Embarked       1307 non-null object
    Fare           1308 non-null float64
    Name           1309 non-null object
    Parch          1309 non-null int64
    PassengerId    1309 non-null int64
    Pclass         1309 non-null int64
    Sex            1309 non-null object
    SibSp          1309 non-null int64
    Survived       891 non-null float64
    Ticket         1309 non-null object
    dtypes: float64(3), int64(4), object(5)
    memory usage: 132.9+ KB



```python
import re
data['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\.', x).group(1))
data.Title.value_counts()
```




    Mr          757
    Miss        260
    Mrs         197
    Master       61
    Rev           8
    Dr            8
    Col           4
    Ms            2
    Major         2
    Mlle          2
    Jonkheer      1
    Capt          1
    Mme           1
    Lady          1
    Don           1
    Sir           1
    Countess      1
    Dona          1
    Name: Title, dtype: int64




```python
#We can replace these titles with only 6 titles (Mr, Mrs, Miss, Master, other, Military and Nobility)
data['Title'] = data['Title'].replace(
{
    'Mile':'Miss', 'Ms':'Miss', 'Mlle':'Miss',
    'Mme':'Mrs', 'Dona':'Mrs',
    'Don':'Mr',
    'Jonkheer':'Nobility', 'Lady':'Nobility', 'Sir':'Nobility', 'Countess':'Nobility',
    'Capt':'Military', 'Major':'Military', 'Col':'Military',
    'Rev':'Other', 'Dr':'Other'
})
data['Title'].value_counts()
```




    Mr          758
    Miss        264
    Mrs         199
    Master       61
    Other        16
    Military      7
    Nobility      4
    Name: Title, dtype: int64



#### Create Family and Alone from Sibsp and Parch


```python
data['Family'] = data['SibSp'] + data['Parch'] + 1
```


```python
#initialise 'Alone' passenger as 1. If he/she has a family, then 'Alone' 0
data['Alone'] = 1
data['Alone'].loc[data['Family'] > 1] = 0
```

    C:\Users\Lenovo\Anaconda3\lib\site-packages\pandas\core\indexing.py:205: SettingWithCopyWarning:
    A value is trying to be set on a copy of a slice from a DataFrame

    See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy



#### Edit Ticket


```python
data['Ticket_1'] = data['Ticket'].map(lambda x: re.sub('\D', '', x))
data['Ticket_1'] = pd.to_numeric(data['Ticket_1'])

ticket=dict(data['Ticket'].value_counts())
data['Ticket_2'] = data['Ticket'].map(ticket)
```

#### Binning Fare and Age


```python
data['Bin_Fare'] = pd.qcut(data.Fare, q=4, labels=False)
data['Bin_Age'] = pd.qcut(data.Age, q=10, labels=False)
```


```python
#create women and children: if parch=1,

#data['children'] = np.where(data['Age']<=15, 1, 0)
#data['elderly'] = np.where(data['Age']>=70, 1, 0)

#data['fragile'] = data['children'] + data['elderly']
```

#### Create Deck and Has_Cabin from Cabin


```python
data['Deck'] = data['Cabin'].str.get(0)
data['Has_Cabin'] = ~data.Cabin.isnull() #
data['Has_Cabin'] = data.Has_Cabin.astype(int)
data.Has_Cabin.value_counts()
```




    0    1014
    1     295
    Name: Has_Cabin, dtype: int64




```python
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Cabin</th>
      <th>Embarked</th>
      <th>Fare</th>
      <th>Name</th>
      <th>Parch</th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>SibSp</th>
      <th>...</th>
      <th>Ticket</th>
      <th>Title</th>
      <th>Family</th>
      <th>Alone</th>
      <th>Ticket_1</th>
      <th>Ticket_2</th>
      <th>Bin_Fare</th>
      <th>Bin_Age</th>
      <th>Deck</th>
      <th>Has_Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>22.0</td>
      <td>NaN</td>
      <td>S</td>
      <td>7.2500</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>male</td>
      <td>1</td>
      <td>...</td>
      <td>A/5 21171</td>
      <td>Mr</td>
      <td>2</td>
      <td>0</td>
      <td>521171.0</td>
      <td>1</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>38.0</td>
      <td>C85</td>
      <td>C</td>
      <td>71.2833</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>female</td>
      <td>1</td>
      <td>...</td>
      <td>PC 17599</td>
      <td>Mrs</td>
      <td>2</td>
      <td>0</td>
      <td>17599.0</td>
      <td>2</td>
      <td>3.0</td>
      <td>7.0</td>
      <td>C</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>26.0</td>
      <td>NaN</td>
      <td>S</td>
      <td>7.9250</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>0</td>
      <td>3</td>
      <td>3</td>
      <td>female</td>
      <td>0</td>
      <td>...</td>
      <td>STON/O2. 3101282</td>
      <td>Miss</td>
      <td>1</td>
      <td>1</td>
      <td>23101282.0</td>
      <td>1</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>35.0</td>
      <td>C123</td>
      <td>S</td>
      <td>53.1000</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>0</td>
      <td>4</td>
      <td>1</td>
      <td>female</td>
      <td>1</td>
      <td>...</td>
      <td>113803</td>
      <td>Mrs</td>
      <td>2</td>
      <td>0</td>
      <td>113803.0</td>
      <td>2</td>
      <td>3.0</td>
      <td>6.0</td>
      <td>C</td>
      <td>1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>35.0</td>
      <td>NaN</td>
      <td>S</td>
      <td>8.0500</td>
      <td>Allen, Mr. William Henry</td>
      <td>0</td>
      <td>5</td>
      <td>3</td>
      <td>male</td>
      <td>0</td>
      <td>...</td>
      <td>373450</td>
      <td>Mr</td>
      <td>1</td>
      <td>1</td>
      <td>373450.0</td>
      <td>1</td>
      <td>1.0</td>
      <td>6.0</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>



### 3.3. Drop Features


```python
data.columns
data = data.drop(['Alone', 'Parch', 'Name', 'PassengerId', 'Survived', 'Ticket', 'Cabin','Fare', 'Bin_Fare', 'Bin_Age', 'Ticket_1', 'Deck'], axis=1)
```

### 3.4. Feature Transform


```python
train = data.iloc[:891]
test = data.iloc[891:]
train.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Embarked</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>SibSp</th>
      <th>Title</th>
      <th>Family</th>
      <th>Ticket_2</th>
      <th>Has_Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>22.0</td>
      <td>S</td>
      <td>3</td>
      <td>male</td>
      <td>1</td>
      <td>Mr</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>38.0</td>
      <td>C</td>
      <td>1</td>
      <td>female</td>
      <td>1</td>
      <td>Mrs</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>26.0</td>
      <td>S</td>
      <td>3</td>
      <td>female</td>
      <td>0</td>
      <td>Miss</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>35.0</td>
      <td>S</td>
      <td>1</td>
      <td>female</td>
      <td>1</td>
      <td>Mrs</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>35.0</td>
      <td>S</td>
      <td>3</td>
      <td>male</td>
      <td>0</td>
      <td>Mr</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
train_num = train.select_dtypes(include=[np.number])
train_num.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Pclass</th>
      <th>SibSp</th>
      <th>Family</th>
      <th>Ticket_2</th>
      <th>Has_Cabin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>22.0</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>26.0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>35.0</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>35.0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
from sklearn.pipeline import Pipeline as pl
from sklearn.impute import SimpleImputer as si
from sklearn.preprocessing import RobustScaler

num_pipeline = pl([
    ('imputer', si(strategy='median')),
    ('scaler', RobustScaler()),
])

train_num_trx = num_pipeline.fit_transform(train_num)
train_num_trx
```


```python
train_cat = train.select_dtypes(exclude=[np.number])
train_cat.head()
```


```python
from sklearn.preprocessing import OneHotEncoder as onehot

cat_pipeline = pl([
    ('imputer', si(strategy='most_frequent')),
    ('encoder', onehot(sparse=False, drop='first')),
])
train_cat_trx = cat_pipeline.fit_transform(train_cat)
train_cat_trx
```


```python
from sklearn.compose import ColumnTransformer as colt

num_attribs = list(train_num)
cat_attribs = list(train_cat)

full_pipeline = colt([
    ('num', num_pipeline, num_attribs),
    ('cat', cat_pipeline, cat_attribs),
])
```

### 3.5. Recheck and Rename


```python
print(train_cat_trx.shape)
print(train_num_trx.shape)
```


```python
X_train = full_pipeline.fit_transform(train)
```


```python
X_test = full_pipeline.transform(test)
```


```python
#Check for missing values  for X_train and X_test
```


```python
y_train = train_df['Survived']
```


```python
y_test = submission_df['Survived']
```

## 4. Modelling
Adaboost decisiontree, random_forest and gradientboosting


```python
from sklearn.ensemble import RandomForestClassifier as rfc


forest = rfc(random_state=0, n_estimators=500, max_leaf_nodes=16)
forest.fit(X_train, y_train)
forest_ts = forest.predict(X_test)
forest_ts
```


```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')
lda.fit(X_train, y_train)
lda_ts = lda.predict(X_test)
```


```python
from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(random_state=0, n_estimators=500)
gbc.fit(X_train, y_train)
gbc_ts = gbc.predict(X_test)
```


```python
from sklearn.linear_model import LogisticRegression

log = LogisticRegression(max_iter=500, random_state=0)
log.fit(X_train, y_train)
log_ts = log.predict(X_test)
```


```python
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier

svc = BaggingClassifier(
    SVC(C=0.7, gamma='auto', random_state=0), random_state=0, n_estimators=500)

svc.fit(X_train, y_train)
svc_ts = svc.predict(X_test)
```

#### let us see how they all performed


```python
from sklearn.metrics import accuracy_score

for clf in (forest, lda, gbc, log, svc):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_pred, y_test))
```

## 5. Evaluation
 Lets see how our model would perform without fitting to the data


```python
from sklearn.ensemble import VotingClassifier

voting_clf = VotingClassifier(
    estimators=[('rfc',forest), ('svc', svc), ('gbc', gbc)], voting='hard')

voting_clf.fit(X_train, y_train)
```


```python
from sklearn.model_selection import cross_val_score

forest_scores = cross_val_score(forest,  X_train, y_train, cv=5)
print(forest_scores.mean())

lda_scores = cross_val_score(lda, X_train, y_train, cv=5)
print(lda_scores.mean())

gbc_scores = cross_val_score(gbc, X_train, y_train, cv=5)
print(gbc_scores.mean())

log_scores = cross_val_score(log,  X_train, y_train, cv=5)
print(log_scores.mean())

svc_scores = cross_val_score(svc,  X_train, y_train, cv=5)
print(svc_scores.mean())

vot_scores = cross_val_score(voting_clf,  X_train, y_train, cv=5)
print(vot_scores.mean())
```

### 5.1. Feature Importance


```python
names = list(train)
print ("Features sorted by their score:")
print (sorted(zip(map(lambda x: round(x, 4), forest.feature_importances_), names),
             reverse=True))
```

## 6. Submision
Select best achieving model and submit


```python
y_pred = voting_clf.predict(X_test)
```


```python
output = pd.DataFrame({'PassengerId':submission_df.PassengerId, 'Survived': y_pred})
```


```python
output.to_csv('vote.csv', index=False)
```


```python
output.head()
```


```python

```
